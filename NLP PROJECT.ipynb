{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7303c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91016208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and extract main content text from a URL\n",
    "def extract_main_content_text(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)  # Added timeout to handle slow responses\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Removing header and footer (this may vary depending on the structure of the websites)\n",
    "        for tag in soup.find_all(['header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "\n",
    "        # Extract text from the remaining content\n",
    "        main_content = soup.get_text(separator=' ', strip=True)\n",
    "        return main_content\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"Error fetching URL\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290c1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file to get the URLs\n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\Ankit Rai\\Downloads\\Input.xlsx\")\n",
    "\n",
    "# Assuming the URLs are in a column named 'URL'\n",
    "urls = df['URL']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2563f726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1      https://insights.blackcoffer.com/streamlined-i...\n",
       "2      https://insights.blackcoffer.com/efficient-dat...\n",
       "3      https://insights.blackcoffer.com/effective-man...\n",
       "4      https://insights.blackcoffer.com/streamlined-t...\n",
       "                             ...                        \n",
       "142    https://insights.blackcoffer.com/population-an...\n",
       "143    https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144    https://insights.blackcoffer.com/healthcare-da...\n",
       "145    https://insights.blackcoffer.com/budget-sales-...\n",
       "146    https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "Name: URL, Length: 147, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3adc49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from each URL and store in a list\n",
    "extracted_texts = []\n",
    "for url in urls:\n",
    "    text = extract_main_content_text(url)\n",
    "    extracted_texts.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b0ebab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>ML and AI-based insurance premium model to pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>Streamlined Integration: Interactive Brokers A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>Efficient Data Integration and User-Friendly I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>Effective Management of Social Media Data Extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>Streamlined Trading Operations Interface for M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bctech2153</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "      <td>Population and Community Survey of America - B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bctech2154</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "      <td>Google LSA API Data Automation and Dashboardin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>bctech2155</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "      <td>Healthcare Data Analysis - Blackcoffer Insight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bctech2156</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "      <td>Budget, Sales KPI Dashboard using Power BI - B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bctech2157</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "      <td>Amazon Buy Bot, an Automation AI tool to Auto-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         URL_ID                                                URL  \\\n",
       "0    bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1    bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2    bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3    bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4    bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "..          ...                                                ...   \n",
       "142  bctech2153  https://insights.blackcoffer.com/population-an...   \n",
       "143  bctech2154  https://insights.blackcoffer.com/google-lsa-ap...   \n",
       "144  bctech2155  https://insights.blackcoffer.com/healthcare-da...   \n",
       "145  bctech2156  https://insights.blackcoffer.com/budget-sales-...   \n",
       "146  bctech2157  https://insights.blackcoffer.com/amazon-buy-bo...   \n",
       "\n",
       "                                        Extracted_Text  \n",
       "0    ML and AI-based insurance premium model to pre...  \n",
       "1    Streamlined Integration: Interactive Brokers A...  \n",
       "2    Efficient Data Integration and User-Friendly I...  \n",
       "3    Effective Management of Social Media Data Extr...  \n",
       "4    Streamlined Trading Operations Interface for M...  \n",
       "..                                                 ...  \n",
       "142  Population and Community Survey of America - B...  \n",
       "143  Google LSA API Data Automation and Dashboardin...  \n",
       "144  Healthcare Data Analysis - Blackcoffer Insight...  \n",
       "145  Budget, Sales KPI Dashboard using Power BI - B...  \n",
       "146  Amazon Buy Bot, an Automation AI tool to Auto-...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Add the extracted text to the DataFrame\n",
    "df['Extracted_Text'] = extracted_texts\n",
    "pd.DataFrame(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171594a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a new csv file\n",
    "df.to_csv(\"C:/Users/Ankit Rai/OneDrive/Desktop.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b929048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmudictNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading cmudict-1.0.26-py3-none-any.whl (939 kB)\n",
      "     -------------------------------------- 939.4/939.4 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting importlib-resources>=5\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting importlib-metadata>=5\n",
      "  Downloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ankit rai\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict) (3.11.0)\n",
      "Installing collected packages: importlib-resources, importlib-metadata, cmudict\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed cmudict-1.0.26 importlib-metadata-8.0.0 importlib-resources-6.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb2b85a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\Ankit\n",
      "[nltk_data]     Rai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ankit\n",
      "[nltk_data]     Rai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ankit Rai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the cmudict for syllable count\n",
    "d = cmudict.dict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6c15ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0] if word.lower() in d else len(re.findall(r'[aeiouy]+', word.lower()))\n",
    "\n",
    "def count_complex_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if count_syllables(word) >= 3]\n",
    "    return len(complex_words)\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Tokenize text\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Calculate word count\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # Calculate sentence length\n",
    "    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "    \n",
    "    # Calculate complex words\n",
    "    complex_word_count = count_complex_words(text)\n",
    "    percentage_complex_words = complex_word_count / word_count * 100\n",
    "    \n",
    "    # Calculate fog index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    # Calculate syllables per word\n",
    "    syllables_per_word = sum(count_syllables(word) for word in words) / word_count\n",
    "    \n",
    "    # Calculate personal pronouns\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    \n",
    "    ## average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count\n",
    "    \n",
    "    ## polarity and subjectivity\n",
    "    blob = TextBlob(text)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    \n",
    "    ## Positive Negative\n",
    "    positive_score = sum(blob.sentiment.polarity > 0 for word in words)\n",
    "    negative_score = sum(blob.sentiment.polarity < 0 for word in words)\n",
    "    \n",
    "    return {\n",
    "        \"POSITIVE_SCORE\": positive_score,\n",
    "        \"NEGATIVE_SCORE\": negative_score,\n",
    "        \"POLARITY_SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY_SCORE\": subjectivity_score,\n",
    "        \"AVG_SENTENCE_LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE_OF_COMPLEX_WORDS\": percentage_complex_words,\n",
    "        \"FOG_INDEX\": fog_index,\n",
    "        \"AVG_NUMBER_OF_WORDS_PER_SENTENCE\": avg_sentence_length,\n",
    "        \"COMPLEX_WORD_COUNT\": complex_word_count,\n",
    "        \"WORD_COUNT\": word_count,\n",
    "        \"SYLLABLE_PER_WORD\": syllables_per_word,\n",
    "        \"PERSONAL_PRONOUNS\": personal_pronouns,\n",
    "        \"AVG_WORD_LENGTH\": avg_word_length\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## File for analysis\n",
    "analysis_data = pd.read_csv(\"C:/Users/Ankit Rai/OneDrive/Desktop.csv\")\n",
    "\n",
    "## text analysis to each extracted text\n",
    "analysis_results = analysis_data['Extracted_Text'].apply(analyze_text)\n",
    "analysis_results_df = pd.DataFrame(analysis_results.tolist())\n",
    "\n",
    "# Final results with URL_ID\n",
    "final_results = pd.concat([analysis_data[['URL_ID']], analysis_results_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f52c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_csv(\"C:/Users/Ankit Rai/OneDrive/Final_Result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f97a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
